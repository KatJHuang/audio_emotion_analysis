{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Emotions From Audio With CNN\n",
    "\n",
    "In this notebook I propose a very simple CNN architecture to classify audio clips by the emotion it contains.\n",
    "\n",
    "The training pipeline works as follows:\n",
    "- Audio clips are first converted into spectrograms, which are essentially 2D arrays, and downsampled to save training time\n",
    "- The downsampled spectrograms are then fed to a two-layer vanilla convolutional neural network. The output is probability of labels computed by a softmax operation. \n",
    "\n",
    "The convolutional neural network is chosen because of its proficiency in learning both higher and lower level image features. When an audio sample is represented as a spectrogram, it is essentially an image and we can easily visualize features such as prosodies and intonations. These features are very useful in classifying emotions and the CNN architecture is very good at learning them. The CNN architecture is also robust against variations in audio quality, such as the pitch of the speaker.\n",
    "\n",
    "The dataset here is the speech section of the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). The audio clips covers short speeches intoning 2 sentences with 8 different emotions by a female speaker and a male speaker. The dataset is found at https://zenodo.org/record/1188976#.W2R6RtVKick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "from pathlib import Path\n",
    "import re \n",
    "\n",
    "audio_root_dir = Path(r'./Audio_Speech_Actors_01-24')\n",
    "audio_file_pattern = Path(r'**/*.wav')\n",
    "\n",
    "def get_emotion_label(filename):\n",
    "    \"\"\"\n",
    "    Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier \n",
    "    (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: \n",
    "\n",
    "    Filename identifiers \n",
    "\n",
    "    Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "    Vocal channel (01 = speech, 02 = song).\n",
    "    Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "    Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "    Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "    Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "    Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "    \n",
    "    Here we will only use 'Emotion' as the label for our training\n",
    "    \n",
    "    INPUT\n",
    "        filename\n",
    "        \n",
    "    OUTPUT\n",
    "        emotion label, STARTING FROM 0 AS OPPOSED TO 1\n",
    "    \"\"\"\n",
    "    EMOTION_LABEL_POS = 2 \n",
    "    return int(re.findall(r\"\\d+\", os.path.basename(filename))[EMOTION_LABEL_POS]) - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a few util functions to compute the spectrogram from a WAV file and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import librosa.display \n",
    "import numpy as np\n",
    "\n",
    "# Define a function which wil apply a butterworth bandpass filter\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(samples, lowcut, highcut, sample_rate, order=5):\n",
    "    \"\"\"\n",
    "    Butterworth's filter\n",
    "    \"\"\"\n",
    "    def butter_bandpass(lowcut, highcut, sample_rate, order=5):\n",
    "        nyq = 0.5 * sample_rate\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        return b, a\n",
    "    \n",
    "    b, a = butter_bandpass(lowcut, highcut, sample_rate, order=order)\n",
    "    y = lfilter(b, a, samples)\n",
    "    return y\n",
    "\n",
    "def clean_audio(samples, sample_rate, lowcut=30, highcut=3000):\n",
    "    \"\"\"\n",
    "    return a preprocessed waveform with normalized volumn, bandpass filtered to only\n",
    "    contain freq range of human speech, and trimmed of starting and trailing silence\n",
    "    \n",
    "    INPUT\n",
    "        samples       1D array containing volumns at different time\n",
    "        sample_rate\n",
    "        lowcut        lower bound for the bandpass filter, default to 30Hz\n",
    "        highcut       higher bound for the bandpass filter, default to 3000Hz\n",
    "    \n",
    "    OUTPUT\n",
    "        filtered      1D array containing preprocessed audio information\n",
    "    \"\"\"\n",
    "    # remove silence at the start and end of \n",
    "    trimmed, index = librosa.effects.trim(samples)\n",
    "    # only keep frequencies common in human speech\n",
    "    filtered = butter_bandpass_filter(samples, lowcut, highcut, sample_rate, order=5)\n",
    "    return filtered\n",
    "\n",
    "def get_melspectrogram(audio_path):\n",
    "    \"\"\"\n",
    "    return a denoised spectrogram of audio clip given path\n",
    "    \n",
    "    INPUT\n",
    "        audio_path    string\n",
    "    OUTPUT\n",
    "        spectrogram   2D array, where axis 0 is time and axis 1 is fourier decomposition\n",
    "                      of waveform at different times\n",
    "    \"\"\"\n",
    "    samples, sample_rate = librosa.load(audio_file_path)\n",
    "    samples = clean_audio(samples, sample_rate)\n",
    "    \n",
    "    melspectrogram = librosa.feature.melspectrogram(samples, sample_rate) \n",
    "    \n",
    "    # max L-infinity normalized the energy \n",
    "    return librosa.util.normalize(melspectrogram)\n",
    "     \n",
    "def display_spectrogram(melspectrogram):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(melspectrogram,\n",
    "                             y_axis='mel', \n",
    "                             fmax=8000,\n",
    "                             x_axis='time')\n",
    "    \n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('melspectrogram')\n",
    "    plt.show()\n",
    "    \n",
    "def align_and_downsample(spectrogram, max_freq_bins=128, max_frames=150, freq_strides=1, frame_strides=1):\n",
    "    return spectrogram[:max_freq_bins:freq_strides, :max_frames:frame_strides]\n",
    "\n",
    "def duplicate_and_stack(layer, dups=3):\n",
    "    return np.stack((layer for _ in range(dups)), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example of a spectrogram. This is the input to the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio_file_path = 'Audio_Speech_Actors_01-24/Actor_07/03-01-01-01-01-02-07.wav'\n",
    "\n",
    "spectrogram = get_melspectrogram(audio_file_path)\n",
    "display_spectrogram(spectrogram)\n",
    "\n",
    "spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain all audio files, convert them into spectrograms, and extract the emotion labels from the file names. Currently we are only trying to classify anger, therefore all other emotion labels are combined into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spectrograms = []\n",
    "labels = []\n",
    "\n",
    "# takes about 6-8 min on my machine\n",
    "counter = 0\n",
    "for audio_file in glob.iglob(str(audio_root_dir / audio_file_pattern), recursive=True):\n",
    "    labels.append(get_emotion_label(audio_file))\n",
    "    \n",
    "    spectrogram = get_melspectrogram(audio_file)\n",
    "    spectrograms.append(duplicate_and_stack(align_and_downsample(spectrogram)))\n",
    "    \n",
    "    if counter % 100 == 0:\n",
    "        print('Processing the {}th file: {}'.format(counter, audio_file))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels_dict = dict(zip(range(8), \n",
    "                       ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']))\n",
    "df = pd.DataFrame(labels, columns=['label'])\n",
    "df.replace({\"label\": labels_dict}, inplace=True)\n",
    "df['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spectrograms = np.array(spectrograms)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectrograms, labels, test_size=0.4, random_state=0)\n",
    "print('X_train.shape = {}'.format(X_train.shape))\n",
    "print('y_train.shape = {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define CNN architecture here. We have two conv/max pooling layers followed by a dense layer with dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    _, height, width, _ = features.shape\n",
    "    height, width = int(height), int(width)\n",
    "    \n",
    "    kernel_size=[5, 5]\n",
    "    \n",
    "    strides = 5\n",
    "    pool_size = [5, 5]\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=features,\n",
    "      filters=32,\n",
    "      kernel_size=kernel_size,\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=pool_size, strides=strides)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2_filters = 64\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=conv2_filters,\n",
    "      kernel_size=kernel_size,\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=pool_size, strides=strides)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, \n",
    "                            [-1, (height // (strides ** 2)) * (width // (strides ** 2)) * conv2_filters])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "emotion_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/audio_cnn_model\")\n",
    "\n",
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "emotion_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=5000,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x=X_test,\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = emotion_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "# If you are only interested in convolution filters. Note that by not\n",
    "# specifying the shape of top layers, the input tensor shape is (None, None, 3),\n",
    "# so you can use them for any size of images.\n",
    "vgg_model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# If you want to specify input tensor\n",
    "from keras.layers import Input\n",
    "input_tensor = Input(shape=(128, 150, 3))\n",
    "vgg_model = applications.VGG16(weights='imagenet',\n",
    "                               include_top=False,\n",
    "                               input_tensor=input_tensor)\n",
    "\n",
    "# To see the models' architecture and layer names, run the following\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "num_classes = 8\n",
    "\n",
    "# Creating dictionary that maps layer names to the layers\n",
    "layer_dict = dict([(layer.name, layer) for layer in vgg_model.layers])\n",
    "\n",
    "# Getting output tensor of the last VGG layer that we want to include\n",
    "x = layer_dict['block1_pool'].output\n",
    "\n",
    "# Stacking a new two layer neural network on top of it \n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Creating new model. Please note that this is NOT a Sequential() model.\n",
    "from keras.models import Model\n",
    "custom_model = Model(input=vgg_model.input, output=x)\n",
    "\n",
    "# Make sure that the pre-trained bottom layers are not trainable\n",
    "for layer in custom_model.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Do not forget to compile it\n",
    "custom_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=keras.optimizers.Adadelta(),\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "from pathlib import Path\n",
    "\n",
    "log_dir = str(Path('./Graph'))\n",
    "tbCallBack = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_test.shape)\n",
    "\n",
    "custom_model.fit(x=X_train, \n",
    "                 y=y_train, \n",
    "                 epochs=20, \n",
    "                 verbose=1, \n",
    "                 callbacks=[tbCallBack],\n",
    "                 validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "tensorboard_cmd = 'tensorboard --logdir {}'.format(log_dir)\n",
    "subprocess.run(tensorboard_cmd.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = custom_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "sentiment_analysis_venv",
   "language": "python",
   "name": "sentiment_analysis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
